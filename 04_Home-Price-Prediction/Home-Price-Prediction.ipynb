{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"W207_Final_Kaggle_House_Prices.ipynb","provenance":[{"file_id":"1PBSqpTY3Gz0NFTMYo3TXkMUngPiNJoKL","timestamp":1617193567161}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sZWIVyqzHvAu"},"source":["# Prepare the Environment\n"]},{"cell_type":"code","metadata":{"id":"GltwOuiNHSrN"},"source":["# This tells matplotlib not to try opening a new window for each plot.\n","%matplotlib inline\n","\n","# Import a bunch of libraries.\n","from collections import Counter, OrderedDict\n","import statistics\n","import operator\n","\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.style as style\n","import seaborn as sns\n","import scipy.stats as ss\n","\n","import xgboost\n","import pickle\n","\n","from scipy.stats import norm\n","from sklearn.linear_model import LinearRegression\n","\n","from sklearn.model_selection import GridSearchCV\n","from pandas import read_csv\n","\n","from sklearn.ensemble import IsolationForest\n","import sklearn.metrics\n","\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.linear_model import ElasticNet\n","from sklearn.linear_model import Lasso\n","\n","# flag for verbose printout of lengthy functions\n","verbose = False\n","\n","# flag for submission file creation\n","submission = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GKN1PYHy010I"},"source":[""]},{"cell_type":"code","metadata":{"id":"ijqDheXYzW0S"},"source":["pd.set_option('max_columns', None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i6exxYRKH4Lw"},"source":["## Function to create submission file"]},{"cell_type":"code","metadata":{"id":"6w2bn80CHolE"},"source":["def submission_file(y_pred, filename):\n","\n","  pred=pd.DataFrame(y_pred)\n","  sub_df=pd.read_csv('https://raw.githubusercontent.com/rambles-tech/w207_final_project_kaggle/main/data/sample_submission.csv')\n","\n","  # grab just the ID from sample_submission.csv\n","  datasets=pd.concat([sub_df['Id'],pred],axis=1)\n","  \n","  # append y_pred to newly-created DF\n","  datasets.columns=['Id','SalePrice']\n","  \n","  # write to file\n","  datasets.to_csv(filename,index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tCbTsAmiH8Ux"},"source":["# Bring in the Data"]},{"cell_type":"code","metadata":{"id":"fazOyz1lHt8y"},"source":["train_data = pd.read_csv(\"https://raw.githubusercontent.com/rambles-tech/w207_final_project_kaggle/main/data/train.csv\")\n","test_data = pd.read_csv(\"https://raw.githubusercontent.com/rambles-tech/w207_final_project_kaggle/main/data/test.csv\")\n","\n","# combine train and test data for ease of feature engineering\n","data = pd.concat([train_data, test_data]) #1459 is last training_data entry\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_VabmXaIO_6"},"source":["# EDA"]},{"cell_type":"markdown","metadata":{"id":"yjaQEyOC0Vt_"},"source":["Confirm the shape of the data  \n","*Note: test_data should have one less column (sale_price)*"]},{"cell_type":"code","metadata":{"id":"kS-IshjiioKf"},"source":["print(train_data.shape)\n","print(test_data.shape)\n","print(data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZnkG2ba81RJv"},"source":["## Validate Data\n","\n","The following dictionary is created from the original data description and inference following domain-specific research.  This will be used to check for inconsistent or incorrect values in the dataset."]},{"cell_type":"code","metadata":{"id":"gZdxxAoDt9Zz"},"source":["# feature values\n","feature_values_dict = {\n","    ### NUMERIC FEATURES ###\n","    'LotFrontage': [0,1000000], \n","    'LotArea' : [0,1000000], \n","    'MasVnrArea' : [0, 10000], \n","    'BsmtFinSF1' : [0, 10000], \n","    'BsmtFinSF2' :[0, 10000], \n","    'BsmtUnfSF' : [0, 10000], \n","    'TotalBsmtSF' : [0, 10000], \n","    '1stFlrSF' : [0, 10000], \n","    '2ndFlrSF' : [0, 10000], \n","    'LowQualFinSF' : [0, 10000], \n","    'GrLivArea': [0, 10000], \n","    'BsmtFullBath' : [0, 10],\n","    'BsmtHalfBath' : [0, 10],\n","    'FullBath' : [0, 10], \n","    'HalfBath' : [0, 10], \n","    'BedroomAbvGr' : [0, 10], \n","    'KitchenAbvGr' : [0, 10], \n","    'TotRmsAbvGrd' : [0, 20], \n","    'Fireplaces' : [0, 5], \n","    'GarageCars' : [0,10000], \n","    'GarageArea': [0, 10000], \n","    'WoodDeckSF' : [0, 2000], \n","    'OpenPorchSF' : [0, 2000], \n","    'EnclosedPorch' : [0, 2000], \n","    '3SsnPorch' : [0, 2000], \n","    'ScreenPorch' : [0, 2000], \n","    'PoolArea' : [0, 2000], \n","    'MiscVal' : [0, 50000], \n","    'SalePrice' : [0,1000000],\n","\n","    ### CATEGORICAL and ORDINAL FEATURES ###\n","    'Id' : [1,2919],\n","    'MSSubClass': [20,30,40,45,50,60,70,75,80,85,90,120,150,160,180,190],\n","    'MSZoning': ['A','C','FV', 'I', 'RH','RL', 'RP', 'RM'], \n","    'Street' : ['Grvl', 'Pave'], \n","    'Alley' : ['Grvl', 'Pave', 'NA'], \n","    'LotShape' : ['Reg', 'IR1', 'IR2','IR3'], \n","    'LandContour' : ['Lvl', 'Bnk', 'HLS', 'Low'], \n","    'Utilities' : ['AllPub', 'NoSewr', 'NoSeWa', 'ELO'], \n","    'LotConfig' : ['Inside', 'Corner', 'CulDSac', 'FR2', 'FR3'], \n","    'LandSlope': ['Gtl', 'Mod', 'Sev'], \n","    'Neighborhood' : ['Blmngtn', 'Blueste', 'BrDale', 'BrkSide', 'ClearCr', \n","                      'CollgCr', 'Crawfor', 'Edwards', 'Gilbert', 'IDOTRR',\n","                      'MeadowV', 'Mitchel', 'Names', 'NoRidge', 'NPkVill', \n","                      'NridgHt', 'NWAmes', 'OldTown', 'SWISU', 'Sawyer', \n","                      'SawyerW', 'Somerst', 'StoneBr', 'Timber', 'Veenker'], \n","    'Condition1' : ['Artery', 'Feedr', 'Norm', 'RRNn', 'RRAn', 'PosN', 'PosA', 'RRNe', 'RRAe'], \n","    'Condition2' : ['Artery', 'Feedr', 'Norm', 'RRNn', 'RRAn', 'PosN', 'PosA', 'RRNe', 'RRAe'], \n","    'BldgType' : ['1Fam', '2FmCon', 'Duplx', 'TwnhsE', 'TwnhsI'],\n","    'HouseStyle' : ['1Story', '1.5Fin', '1.5Unf', '2Story', '2.5Fin', '2.5Unf', 'SFoyer', 'SLvl'], \n","    'OverallQual' : [1,2,3,4,5,6,7,8,9,10], \n","    'OverallCond' : [1,2,3,4,5,6,7,8,9,10], \n","    'YearBuilt' : [1850,2010], \n","    'YearRemodAdd' : [1850,2010], \n","    'RoofStyle' : ['Flat', 'Gable', 'Gambrel', 'Hip', 'Mansard', 'Shed'], \n","    'RoofMatl' : ['ClyTile', 'CompShg', 'Membran', 'Metal', 'Roll', 'Tar&Grv', 'WdShake', 'WdShngl'], \n","    'Exterior1st' : ['AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock' , 'CemntBd', 'HdBoard', 'ImStucc', 'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco', 'VinylSd', 'Wd Sdng', 'WdShing'], \n","    'Exterior2nd' : ['AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock' , 'CemntBd', 'HdBoard', 'ImStucc', 'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco', 'VinylSd', 'Wd Sdng', 'WdShing'],\n","    'MasVnrType' : ['BrkCmn','BrkFace','CBlock','None','Stone'], \n","    'ExterQual' : ['Ex', 'Gd', 'TA', 'Fa', 'Po'], \n","    'ExterCond' : ['Ex', 'Gd', 'TA', 'Fa', 'Po'], \n","    'Foundation' : ['BrkTil', 'CBlock', 'PConc', 'Slab', 'Stone', 'Wood'], \n","    'BsmtQual' : ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'], \n","    'BsmtCond' : ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'], \n","    'BsmtExposure': ['Gd', 'Av', 'Mn', 'No', 'NA'], \n","    'BsmtFinType1' : ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'NA'],\n","    'BsmtFinType2' : ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'NA'], \n","    'Heating' : ['Floor', 'GasA', 'GasW', 'Grav', 'OthW', 'Wall'], \n","    'HeatingQC' : ['Ex', 'Gd', 'TA', 'Fa', 'Po'], \n","    'CentralAir' : ['N', 'Y'], \n","    'Electrical' : ['SBrkr', 'FuseA', 'FuseF', 'FuseP', 'Mix'], \n","    'KitchenQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po'], \n","    'Functional': ['Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal'], \n","    'FireplaceQu': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'], \n","    'GarageType' : ['2Types', 'Attchd', 'Basment', 'BuiltIn', 'CarPort', 'Detchd', 'NA'], \n","    'GarageYrBlt':[1850, 2010], \n","    'GarageFinish' : ['Fin', 'RFn', 'Unf', 'NA'], \n","    'GarageQual' : ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'], \n","    'GarageCond' : ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'], \n","    'PavedDrive' : ['Y', 'P', 'N'], \n","    'PoolQC' : ['Ex', 'Gd', 'TA', 'Fa', 'NA'], \n","    'Fence': ['GdPrv', 'MnPrv', 'GdWo', 'MnWw', 'NA'], \n","    'MiscFeature': ['Elev', 'Gar2', 'Othr', 'Shed', 'TenC', 'NA'], \n","    'MoSold' : [1,2,3,4,5,6,7,8,9,10,11,12], \n","    'YrSold' : [2006, 2007, 2008, 2009, 2010], \n","    'SaleType' : ['WD', 'CWD', 'VWD', 'New', 'COD', 'Con', 'ConLw', 'ConLI', 'ConLD', 'Oth'],\n","    'SaleCondition' : ['Normal', 'Abnorml', 'AdjLand', 'Alloca', 'Family', 'Partial']\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X5swb8Mg88wL"},"source":["This function will compare vaues of each feature in the dataset against the above-created dictionary of valid ranges/values."]},{"cell_type":"code","metadata":{"id":"qzX01_eHjgPS"},"source":["# examine the feature stats\n","def examine_feature(df,list_features,dict_values):\n","  for feature in list_features:\n","    print(df[feature].head())\n","\n","    # create a dictionary of keys and values\n","    class_keys = Counter(df[feature]).keys()\n","    class_values = Counter(df[feature]).values()\n","    \n","    class_dict = dict(zip(class_keys,class_values))\n","\n","    # sort dict by values\n","    od_values = dict(sorted(class_dict.items(), key=lambda item: item[1]))\n","\n","    # variable to hold the count\n","    cnt = 0\n","    \n","    # list to hold visited values\n","    visited = []\n","\n","    # loop for counting the unique values in feature\n","    for i in range(0, len(df[feature])):\n","      if df[feature].iloc[i] not in visited: \n","        visited.append(df[feature].iloc[i])\n","        cnt += 1\n","\n","    # sort the list of unique visited feature values\n","    visited.sort(key=repr)\n","    if isinstance(data[feature].iloc[0],np.int64) or isinstance(data[feature].iloc[0],np.float64):\n","      print('min =',df[feature].min())\n","      print('max =',df[feature].max())\n","      visited.sort()\n","      \n","    # print('mode =',df[feature].mode())\n","    print('value sort     :',od_values)\n","    print('# of samples   :',len(df))\n","    print('nans =',len(df[df[feature].isna()]))\n","    print('# of uniques   :',cnt)\n","    print('unique values  :',visited)\n","\n","    print('possible values:',sorted(dict_values[feature]))\n","    print('cat not in list:',(list(set(visited)-set(dict_values[feature]))))\n","    print('cat to add as col:',(list(set(dict_values[feature])-set(visited))))\n","    print('*************')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2pSl7ElBCgH"},"source":["# examine feature values \n","if verbose: examine_feature(data,data.columns,feature_values_dict)\n","else: print(\"Output suppressed; set `verbose` to True if output is desired\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSbwHGC4MWcA"},"source":["## Identify Duplicate Entries"]},{"cell_type":"code","metadata":{"id":"qL89919dGQRY"},"source":["# no duplicate id's\n","data.Id.unique().size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2JU8lDicMufJ"},"source":["No duplicate data found"]},{"cell_type":"markdown","metadata":{"id":"6k1T-0vuI77b"},"source":["## Identify Missing Data  "]},{"cell_type":"code","metadata":{"id":"RblaHDZNGatA"},"source":["plt.figure(figsize=(24,8))\n","sns.heatmap(data.isnull(),yticklabels=False,cbar=False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aODNv543KOd7"},"source":["# find NA values, sorted by columns with most NA entries\n","data.isnull().sum(0).sort_values(ascending=False).head(36)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eNhivMd-K9cR"},"source":["## Identify Incorrect Data"]},{"cell_type":"markdown","metadata":{"id":"d54ElKAEL0Qg"},"source":["### The Neighborhood Level\n","\n","Lets compare values at the data-set level compared with the neighborhood level"]},{"cell_type":"code","metadata":{"id":"ofoHczIM4DCM"},"source":["examples = ['LotFrontage', 'LotArea', 'YearBuilt']\n","\n","fig, axes = plt.subplots(1, len(examples))\n","\n","#plt.figure(figsize=(20,10)) \n","fig.suptitle('Comparison between overall data mean (in red) and neighborhood means (in blue)')\n","fig.tight_layout()\n","\n","for i, feature in enumerate(data[examples].columns):\n","  \n","  if data[feature].dtypes == 'int64' or data[feature].dtypes == 'float64' :\n","    axes[i].set_title(feature)\n","    # plot the mean by neighborhood\n","    axes[i].plot( data.groupby('Neighborhood')[feature].mean()  )\n","\n","    # plot the overall mean\n","    axes[i].axhline(y = data[feature].value_counts().idxmax(), color = 'r', linestyle = '-') \n","\n","    axes[i].get_xaxis().set_visible(False)\n","  else:\n","    \n","    print(feature)\n","    print(\"Overall Mode\")\n","    print(data[feature].value_counts().idxmax())\n","\n","    print(\"Neighborhood Modes\")\n","    print(data.groupby('Neighborhood')[feature].value_counts().idxmax())\n","\n","plt.subplots_adjust(left=None, bottom=None, right=None, top=.8, wspace=.8, hspace=None)\n","plt.gcf().set_size_inches(10, 4)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYlE45CaYNhZ"},"source":["**Recommendation: Considering the extreme changes by neighborhood, we recommend using means and modes at the neighborhood level when considering imputing missing values.**"]},{"cell_type":"code","metadata":{"id":"wC39D41ZIXT9"},"source":["data[(data.Neighborhood == 'Blmngtn') & (data.LotArea == 3182)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Ci5wacRK6Uh"},"source":["# Check for standardized values at the neighborhood level\n","data[['Id','Neighborhood','MSSubClass','MSZoning']][data['Neighborhood'] == 'BrkSide']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ovpWRoXa72IE"},"source":["There could potentially be some misclassified `MSSubClass` & `MSZoning` values.  Is it possible the same Neighborhood be both Residencial Medium (RM) and Low (RL) Density?  \n","**Recommendation: Conduct research to determine if houses within the same neighborhood could potentially have differing values.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nRZf2oODMsHM"},"source":["### The House Level"]},{"cell_type":"code","metadata":{"id":"j_KXdeihMwyv"},"source":["data[(data.YearBuilt > 2010) | (data.YearRemodAdd > 2010)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1f21iu3O4srJ"},"source":["There are no houses built or remodeled after 2010."]},{"cell_type":"code","metadata":{"id":"sfN3_XI2M2Gs"},"source":["# Check for illogical sequences\n","data[['Id','YearBuilt','YearRemodAdd','Neighborhood']][data['YearBuilt'] > data['YearRemodAdd']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clND7deI6o-Y"},"source":["Here, we found one case where `YearRemodAdd` was likely incorrectly entered prior to the `YearBuilt`.\n","\n","**Recommendation: Drop or change this value for YearRemodAdd to 2002\n","or add a flat indictor variable for Remodeled**"]},{"cell_type":"code","metadata":{"id":"Z9kp-a1rM6Re"},"source":["# no errors: houses with more than 400 square feet on 2nd floor than the 1st\n","temp_list = data['1stFlrSF'] - data['2ndFlrSF']\n","plt.hist(temp_list,bins=10)\n","plt.show()\n","\n","df_temp = data[['Id','1stFlrSF','2ndFlrSF']]\n","df_temp[((df_temp['1stFlrSF'] - df_temp['2ndFlrSF']) < -400)].head(20)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i1Gsh3KEAUvY"},"source":["It is unclear if this is an error or a factor of architectural design.    \n","**Recommend not changing anything, but certainly be aware of this issue if predictions are not in compliance.**"]},{"cell_type":"code","metadata":{"id":"vudXSVBkM98J"},"source":["# Check for potential misclassifications\n","df_temp = (data[(data.HouseStyle == '1Story') & \n","                        (data['2ndFlrSF'] > 0)])\n","df_temp[['Id','MSSubClass','HouseStyle','1stFlrSF','2ndFlrSF']]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CY3cFzAXBAjB"},"source":["There are three houses which are misclassified as 1 story houses yet they have a 2nd floor.  \n","**Recommendation: Keep.  This could potentially cover a finished attic.  Since they are so few entries with this error, it would be unneccessary to drop these rows.**"]},{"cell_type":"markdown","metadata":{"id":"uoRInlv7Q1a9"},"source":["### The Basement"]},{"cell_type":"code","metadata":{"id":"BsEKZlKVQ8px"},"source":["# counts match\n","print(data[(data.BsmtFinSF1 > 0)].shape[0])\n","print(data[(data.BsmtFinSF2 > 0)].shape[0])\n","#print(data[(data.BsmtFinSF1.isnull())])\n","data[['Id','BsmtFinSF1']].iloc[2120]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"To6tQ7Q5Tpz5"},"source":["There does not seeom to be any discrepancies between the counts of BsmtFinSF1 and BsmtFinSF2.\n","\n","There is at least one NaN value in this feature that must be addressed.\n","\n","**Recommendation: Replace BsmtFinSF1 NaN with neighborhood mean.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BrZXpgOTTSua"},"source":["### The Garage"]},{"cell_type":"code","metadata":{"id":"XfwWQom1TVHj"},"source":["# check for valid garage entries\n","data[['Id','YearBuilt','YearRemodAdd','GarageYrBlt']][(data.GarageYrBlt > 2010)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UtnBHhxIJETT"},"source":["There is one value that is clearly an entry error.  \n","**Recommendation: Either drop this entry, change to YearBuilt to match the house, change to 2007, or replace with neighborhood mean.**\n"]},{"cell_type":"code","metadata":{"id":"-4Nb4AsaTZYT"},"source":["# find garages built before the houses\n","data[['Id','YearBuilt','GarageYrBlt']][(data.GarageYrBlt < data.YearBuilt)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phSnP3P_Jeub"},"source":["**Recommendation: Keep these values.  Without additional research and more data, it is likely not possible to rule these as errors and not a result of a house being demolished while the garage remains.**"]},{"cell_type":"markdown","metadata":{"id":"eB_nU5tgUtN7"},"source":["### The Lot"]},{"cell_type":"code","metadata":{"id":"lrbMhKt3Uv-9"},"source":["# Check various features of the lots\n","depth = (data.LotArea / data.LotFrontage)\n","df_temp = pd.DataFrame(data.Id)\n","df_temp['LotFrontage'] = data.LotFrontage\n","df_temp['depth'] = depth\n","df_temp['LotArea'] = data.LotArea\n","print(df_temp.max())\n","print(df_temp.min())\n","print(df_temp[(df_temp.depth < 75)].head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zF_5wXftJ-UV"},"source":["Nothing significant to report."]},{"cell_type":"code","metadata":{"id":"J5uzuscuUz8s"},"source":["# check for lot plus building total\n","data[['Id','LotArea',\n","         '1stFlrSF','GarageArea','WoodDeckSF',\n","         'OpenPorchSF','EnclosedPorch','ScreenPorch',\n","         'PoolArea']][(data['1stFlrSF'] + data.GarageArea + data.WoodDeckSF + data.OpenPorchSF + data.EnclosedPorch + data.ScreenPorch + data.PoolArea)\n","          > data.LotArea]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4PhLIuU6KGh1"},"source":["There is one entry where the total areas of each component sum greater than the total lot - more square footage of buildings(plus) than the lot.  \n","**Recommendation: Although this entry should be dropped, since it a test subject, the value should be adjusted to equal the sum of components.**"]},{"cell_type":"code","metadata":{"id":"hminh5ZSU3g9"},"source":["# Check for LotFrontAge Issues\n","data[['Id','LotFrontage','LotArea']][(data.LotFrontage < 24)].sort_values(by=['LotArea'], ascending=False).head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sc6GzbYBNcnN"},"source":["Id 2039 LotFrontage and LotArea seem complex and likely is the result of an error.  \n","**Recommendation: This appears to be an extreme outlier, but it is in the test data.  Potentially, this value could be weighted to be more inline with the rest of the samples, but that would not achieve much of an effect.**\n"]},{"cell_type":"markdown","metadata":{"id":"ImkNIIR-GLUK"},"source":["## Sales Price"]},{"cell_type":"code","metadata":{"id":"RVhm3VOoA650"},"source":["# examine the sales price data\n","round(train_data.SalePrice.describe(), 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ER3BPYVvPeXL"},"source":["(mu, sigma) = norm.fit(train_data['SalePrice'])\n","plt.figure(figsize = (12,6))\n","sns.distplot(train_data['SalePrice'], kde = True, hist=True, fit = norm)\n","plt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\n","plt.xlabel(\"House's sale Price in $\", fontsize = 12)\n","plt.legend(['actual price dist','Normal dist ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwrsI32mP8Kq"},"source":["(mu, sigma) = norm.fit(np.log(train_data['SalePrice']))\n","plt.figure(figsize = (12,6))\n","sns.distplot(np.log(train_data['SalePrice']), kde = True, hist=True, fit = norm)\n","plt.title('Log of SalePrice distribution vs Normal Distribution', fontsize = 13)\n","plt.xlabel(\"House's sale Price in log $\", fontsize = 12)\n","plt.legend(['Log price dist','Normal dist ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_gDmaggYPsWE"},"source":["**Recommendation: Use Log-Transform on SalesPrice for Linear Models since it increases the accuracy of the normality assumption.**\n"]},{"cell_type":"markdown","metadata":{"id":"9dIO7K9HZGR8"},"source":["## Consolidated Recommendations\n","\n","- Recommendation: Considering the extreme changes by neighborhood, we recommend using means and modes at the neighborhood level when considering imputing missing values.  **done**  \n","- Recommendation: Drop or change id==1877 value for YearRemodAdd to 2002 or add a flat indictor variable for Remodeled **done**  \n","- Recommendation: Either drop id==2593 entry, change to YearBuilt to match the house, change to 2007, or replace with neighborhood mean.  **unneeded, its test data**  \n","- Recommendation: Drop 2819 **unneeded, its test data**  \n","    - There is one entry where the total areas of each component sum greater than the total lot - more square footage of buildings(plus) than the lot.\n","- Id 2039 LotFrontage and LotArea seem complex and likely is the result of an error. Recommendation: Remove this entry as an outlier.  **unneeded, its test data**  \n","- Recommendation: Use Log-Transform on SalesPrice for Linear Models since it increases the accuracy of the normality assumption.  **done**  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"n5y_lxfP3sSO"},"source":["## Final Recommendations  \n","\n","**Id** - Drop this feature because it does not add any value to the model.  \n","**GarageYrBlt** - Drop feature value typo 2207 outside of possible range (ie. >2010).  \n","\n","---  \n","\n","Rename these features to allow method calls such as \"data.FirstFlrSF\" and  \n","avoid issues with \"data.1stFlrSF\" not working.  \n","**1stFlrSF** - rename to FirstFlrSF  \n","**2ndFlrSF** - rename to SecondFlrSF   \n","**3SsnPorch** - rename to ThreeSsnPorch  \n","\n","---  \n","\n","These features had values that did not match the data descriptions, but do  \n","not have any affect on the current model. These could be changed to stay  \n","consistent if deemed necessary and to avoid errors on future test data.  \n","**MSZoning** - typo  (ie. description = 'C', actual = 'C (all)')  \n","**Neighborhood** - typo  (ie. description = 'Names', actual = 'NAmes')    \n","**BldgType** - typo  (ie. description = '2FmCon', 'TwnhsI', 'Duplx'  \n","$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$\n","actual = '2fmCon', 'Twnhs',  'Duplex')   \n","**Exterior2nd** - typo  (ie. description = 'CemntBd', 'WdShing', 'BrkComm  \n","$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ actual = 'CmentBd', 'Wd Shng', 'Brk Cmn')  \n","\n","---\n","\n","The following features were missing values from the data description  \n","because they did not appear in the training or testing data, but could be  \n","added to avoid erros on future test data.  \n","**MSZoning** - missing 'RP', 'C', 'A', 'I'  \n","**Utilities** - missing 'ELO', 'NoSewr'  \n","**Condition2** - missing 'RRNe'  \n","**OverallCond** - missing '10'  \n","**Exterior1st** - missing 'Other', 'PreCast'  \n","**Exterior2nd** - missing 'PreCast'  \n","**MasVnrType** - missing 'CBlock'  \n","**ExterQual** - missing 'Po'  \n"]},{"cell_type":"markdown","metadata":{"id":"arszp45YIRuR"},"source":["# Cleaning"]},{"cell_type":"markdown","metadata":{"id":"aj_cdL9yX1ir"},"source":["## Drop or Replace Values"]},{"cell_type":"code","metadata":{"id":"goHInGxjX6HA"},"source":["# Recommendation: Replace id==1877 value for YearRemodAdd to 2002\n","data.loc[(data.Id == 1877),'YearRemodAdd'] = 2002\n","\n","# Recommendation: Either drop id==2593 entry, change to YearBuilt to match the house, change to 2007, or replace with neighborhood mean.\n","data.loc[(data.Id == 2593),'GarageYrBlt'] = np.nan"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B1K4DIFqfR04"},"source":["# drop extreme outliers in training data\n","data = data[~data.Id.isin([1298, 523, 1100, 533])]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lOOI59Y16Ctx"},"source":["## Drop Features"]},{"cell_type":"markdown","metadata":{"id":"HObUEAhJVBCc"},"source":["The Id column is artificially correlated and will skew results."]},{"cell_type":"code","metadata":{"id":"dsT_tSF155ma"},"source":["# drop ID \n","data.drop(['Id'],axis=1,inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wG_Izgd1RA7A"},"source":["## Rename Columns"]},{"cell_type":"code","metadata":{"id":"yIWIZibsApxD"},"source":["data.rename(columns = {'1stFlrSF':'FirstFlrSF', '2ndFlrSF':'SecondFlrSF', '3SsnPorch': 'ThreeSsnPorch' }, inplace = True) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Qd2XccgQ-P-"},"source":["## Replace NaNs"]},{"cell_type":"code","metadata":{"id":"6ABwvnFCIV-Q"},"source":["# flag for creating means and modes by neighborhood rather than the entire dataset\n","by_neighborhood = True\n","\n","# replace NaN\n","features_mean = ['LotFrontage', 'GrLivArea'] \n","\n","features_mode = ['MSZoning', 'Utilities', 'Exterior1st','Exterior2nd','MasVnrArea', 'BsmtFinSF1',\n","                 'BsmtFullBath','BsmtHalfBath','KitchenQual','SaleType','Electrical']\n","\n","# replace nans with 'None'\n","features_none = ['Alley','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n","                 'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']\n","\n","# replace nans with 0.0\n","features_zero = ['BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageCars','GarageArea']\n","\n","# replace 'GarageYrBlt' with 'YearBuilt'\n","data['GarageYrBlt'].fillna(data['YearBuilt'], inplace=True)\n","\n","features_spec = ['Functional']\n","\n","for vector in features_mean:\n","\n","  if by_neighborhood:\n","    # good code for neighborhood value-grabbing based on mean\n","    replace_value = data.groupby('Neighborhood')[vector].transform('mean')\n","    data[vector].fillna(replace_value, inplace=True)\n","  \n","  else:\n","    # code for data-wide mean grabbing as opposed to neighborhood specific\n","    mean = data[vector].astype(\"float\").mean(axis=0)\n","    data[vector].replace(np.nan, mean, inplace = True)\n","\n","for vector in features_mode:\n","  if by_neighborhood:\n","    # good code for neighborhood value-grabbing based on mode\n","    data[vector] = data.groupby('Neighborhood')[vector].transform(lambda x: x.fillna(x.mode().iloc[0]))\n","  \n","  else:\n","    # code for data-wide mean grabbing as opposed to neighborhood specific\n","    mode = data[vector].value_counts().idxmax()\n","    data[vector].replace(np.nan, mode, inplace = True)\n","\n","for vector in features_none:\n","  data[vector].replace(np.nan, 'None', inplace = True)\n","\n","for vector in features_zero:\n","  data[vector].replace(np.nan, 0.0, inplace = True)\n","\n","for vector in features_spec:\n","  data[vector].replace(np.nan, 'Typ', inplace = True)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1jbJ47JA9k2y"},"source":["# Analysis"]},{"cell_type":"markdown","metadata":{"id":"JkqSemSvuGbW"},"source":["##Data Info\n","Below is a summary of the dataset's columns, non-null count, and dtype."]},{"cell_type":"code","metadata":{"id":"mXGyBYsytf9y"},"source":["data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RFDEHGyct41Q"},"source":["## Heatmap\n","The heatmap confirms that all Nans have been replaced.  The missing values of the right of the heatmap are the sales prices from the test data."]},{"cell_type":"code","metadata":{"id":"94H2yesCtf2L"},"source":["plt.figure(figsize=(24,8))\n","sns.heatmap(data.isnull(),yticklabels=False,cbar=False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BV-GUnNK_ig3"},"source":["## Correlation Matrix\n","The correlation matrix shows that ten features have a 0.50 or higher correlation with SalePrice.  These variables listed in order from highest correlation to lowest are: OverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF, FirstFlrSF, TotRmsAbvGrd, YearBuilt, and YearRemodAdd.  The overall material and finish of the house is by far the highest correlated feature with sales price at 0.79.\n","\n","Features garage car capacity and garage square feet have the highest correlation at 0.89, which is not surprising.  Other notable features that are highly correlated are total rooms above ground and above grade living area square feet, year home built and year garage built, and first-floor square feet and basement square feet.  \n","\n","Unexpectedly, the overall condition of the house does not have a high correlation with the sales price.  However, after further data exploration, it is determined that the ratings for overall condition are concentrated around five.  From the graph below, it is seen that overall quality has a more normal distribution than overall condition.  Therefore, it is understood why the home's overall condition has a low correlation with the sales price. "]},{"cell_type":"code","metadata":{"id":"uLc0-OSEFBbX"},"source":["# Plot set up for correlation matrix\n","rc = {'font.size': 10, 'axes.labelsize': 16, 'legend.fontsize': 16, 'xtick.labelsize': 16, 'ytick.labelsize':16, }\n","plt.rcParams.update(rc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IVM5gFvGv-YY"},"source":["# Correlation Matrix\n","f, ax = plt.subplots(figsize=(28, 28))\n","mat = data.corr('pearson')\n","mask = np.triu(np.ones_like(mat, dtype=bool))\n","cmap = sns.diverging_palette(230, 20, as_cmap=True)\n","corr = sns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n","            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}).set_title('Correlation Matrix', fontsize=24)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QPrvkPO7K7FB"},"source":["### OverallQual and OverallCond Distributions"]},{"cell_type":"code","metadata":{"id":"j26QLDUiFWe4"},"source":["# Plot set up\n","fig, axes = plt.subplots(1,2, figsize=(20,8))\n","style.use('ggplot')\n","\n","# OverallQual Distirbution\n","overallqual_count = data['OverallQual'].value_counts() \n","sns.barplot(overallqual_count.index, overallqual_count.values, ax=axes[0])\n","axes[0].set_title('Overall Quality Distribution')\n","axes[0].set_ylabel('Counts')\n","axes[0].set_xlabel('Rating')\n","\n","# OverallCond Distribution\n","overallcond_count = data['OverallCond'].value_counts() \n","sns.barplot(overallcond_count.index, overallcond_count.values, ax=axes[1])\n","axes[1].set_title('Overall Condition Distribution')\n","axes[1].set_ylabel('Counts')\n","axes[1].set_xlabel('Rating')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hH7Oj6LELD_q"},"source":["## Exploring OverallQual and GrLivArea"]},{"cell_type":"code","metadata":{"id":"YNCO8-SZ4Oyt"},"source":["# OverallQuall - SalePrice - GrLivArea\n","fig, axes = plt.subplots(1,3, figsize=(28,8))\n","\n","fig.suptitle(\"Sales Price, Overall Quality, Above Grade Living Area\")\n","sns.set_style(\"whitegrid\")\n","\n","# Box plot of OverallQual, SalePrice, and GrLivArea\n","sns.boxplot(data=train_data,x='OverallQual',y='SalePrice', hue=pd.cut(train_data[\"GrLivArea\"], 4), ax=axes[0])\n","axes[0].set_title(\"Boxplot\")\n","\n","# Strip plot of OverallQual, SalePrice, and GrLivArea\n","sns.stripplot(data=train_data,x='OverallQual',y='SalePrice', hue=pd.cut(train_data[\"GrLivArea\"], 4), ax=axes[1])\n","axes[1].set_title(\"Strip Plot\")\n","\n","# Violin plot of OverallQual, SalePrice, and GrLivArea\n","sns.violinplot(data=train_data,x='OverallQual',y='SalePrice', hue=pd.cut(train_data[\"GrLivArea\"], 4), ax=axes[2])\n","axes[2].set_title(\"Violin Plot\")\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_hYFEqXDjsT"},"source":["# Reset rcParams to default \n","plt.rcParams.update(plt.rcParamsDefault)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6AOK6LYIkaN"},"source":["# CONFIRM ALL NA VALUES ARE GONE (SALE PRICE SHOULD BE == 1459)\n","# data.isnull().sum(0).sort_values(ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xUm6-UoRRF19"},"source":["## Column Data Types"]},{"cell_type":"code","metadata":{"id":"_NM6rp2qIn9F"},"source":["  num_colums = ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageYrBlt', \n","  'GarageCars', 'GarageArea', 'SalePrice', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'FirstFlrSF', 'SecondFlrSF', 'LowQualFinSF', 'GrLivArea',\n","  'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n","  'ThreeSsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n","\n","  cat_columns=['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', \n","              'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n","              'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n","              'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', \n","              'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n","\n","  for col in cat_columns:\n","      data[col] = data[col].astype('object')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMrkJji2Pl2v"},"source":["# Turn all variables (columns) into dummy variables\n","def onehot_encoding(df, categorical_columns):\n","\n","  valid_cat_columns = []\n","  # create new df\n","  new_df = df\n","\n","  # remove columns that are not in cat_columns\n","  for col in categorical_columns:\n","    if col in df.columns: \n","      valid_cat_columns.append(col)\n","  \n","  for i, col in enumerate(valid_cat_columns):\n","\n","      # create dummies from column\n","      temp_df = pd.get_dummies(data[col], prefix=col, prefix_sep='_')\n","      \n","      # drop column from df\n","      df.drop([col],axis=1,inplace=True)\n","\n","      # for first column, create new DF\n","      if i==0:\n","          new_df = temp_df.copy()\n","\n","      # for subsequent columns, concat DFs\n","      else:\n","        new_df = pd.concat([new_df,temp_df],axis=1)\n","\n","  # concat DF (with original columns not provided in columns list) with the dummy variables    \n","  new_df=pd.concat([df,new_df],axis=1)\n","      \n","  return new_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FT2-HzPvQ1St"},"source":["# Split data (train, dev, test)\n","\n","Use this line to get fresh data for each model, split into train, dev, test:\n","\n","\n","```\n","train_vectors, dev_vectors, train_SalePrice, dev_SalePrice, test_vectors = get_data(data, False)\n","```\n","\n","Otherwise, just use\n","\n","```\n","train_vectors, dev_vectors, train_SalePrice, dev_SalePrice, test_vectors = get_data(data, True)\n","```\n","to get train, test data to create a submission file.\n"]},{"cell_type":"code","metadata":{"id":"ZJIOWbmCPnMl"},"source":["def get_data(data_frame, submission_flag=submission):\n","  all_data_frame = data_frame.copy()\n","  \n","  # Set the randomizer seed so results are the same each time.\n","  np.random.seed(0)\n","\n","  # onehot_encode dataframe based off previously-created cat_columns \n","  # this ensures that if a dataframe with features removed was passed into this function\n","  # only those columns still in the dataframe are encoded\n","  all_data_frame = onehot_encoding(all_data_frame, cat_columns)\n","\n","  # Seperate data\n","  train_data = all_data_frame.loc[(all_data_frame['SalePrice'].notnull())]\n","  test_data = all_data_frame.loc[(all_data_frame['SalePrice'].isna())]\n","\n","  # remove SalePrice from Train and dev data\n","  train_vectors = train_data.drop('SalePrice', axis = 1)\n","\n","  # create label vector for train and dev data\n","  train_SalePrice = train_data['SalePrice']\n","\n","  # remove SalePrice from Test data\n","  test_vectors = test_data.drop('SalePrice', axis = 1)\n","\n","  # if submission_file is ON, create  ONLY train and test sets\n","  if submission_flag:\n","    return train_vectors, train_SalePrice, test_vectors\n","\n","  # if submission_file is OFF, create train, dev, test sets\n","  else:\n","    # Create filter to split train and dev data (80% train, 20% dev)\n","    msk = np.random.rand(len(train_data)) < 0.8\n","\n","    # remove SalePrice from Train and dev data\n","    train_vectors = train_data[msk].drop('SalePrice', axis = 1)\n","    dev_vectors = train_data[~msk].drop('SalePrice', axis = 1)\n","\n","    # create label vector for train and dev data\n","    train_SalePrice = train_data[msk]['SalePrice']\n","    dev_SalePrice = train_data[~msk]['SalePrice']\n","\n","    # create label vector for train and dev data\n","    train_SalePrice = train_data[msk]['SalePrice']\n","\n","  return train_vectors, dev_vectors, train_SalePrice, dev_SalePrice, test_vectors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIYHFi1kQs5t"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"id":"VNH-wqb3L8F8"},"source":["## Linear Model:"]},{"cell_type":"markdown","metadata":{"id":"fqtz_0k2L-RA"},"source":["### Linear Model: All features"]},{"cell_type":"code","metadata":{"id":"VmX-whWUgYE8"},"source":["submission = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJENZBnuPon5"},"source":["if submission:\n","  train_vectors, train_SalePrice, test_vectors = get_data(data, True)\n","\n","  reg = LinearRegression()\n","  reg.fit(train_vectors, np.log(train_SalePrice))\n","  y_pred = reg.predict(test_vectors)\n","  submission_file(np.exp(y_pred), 'linear_submission_log.csv')\n","\n","train_vectors, dev_vectors, train_SalePrice, dev_SalePrice, test_vectors = get_data(data, False)\n","# Linear Regression, all\n","reg = LinearRegression()\n","reg.fit(train_vectors, np.log(train_SalePrice))\n","\n","y_pred = reg.predict(dev_vectors)\n","\n","print(f\" Score {reg.score(dev_vectors, np.log(dev_SalePrice))}\")\n","print(f\" MSE {sklearn.metrics.mean_squared_error(np.exp(y_pred), dev_SalePrice)}\")\n","print(f\" RMSE {np.sqrt(sklearn.metrics.mean_squared_error(np.exp(y_pred), dev_SalePrice))}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mhVHYYG_60ZL"},"source":["#### Linear Model with all Features\n","Rank: 5979    Score:  15623.551"]},{"cell_type":"code","metadata":{"id":"vwu0-YcuUVKn"},"source":["# Plot set up\n","plt.style.use('ggplot')\n","plt.figure(figsize=(10,8))\n","\n","# Set x and y variable\n","x, y = y_pred, np.log(dev_SalePrice)\n","\n","# Scatter plot - fitted vs. actuals\n","plt.scatter(x, y, alpha=.7,\n","            color='b') #alpha helps to show overlapping data\n","\n","# Line of best fit\n","m, b = np.polyfit(x, y, 1)\n","plt.plot(x, m*x + b)\n","\n","# Plot set up\n","plt.xlabel('Predicted Price')\n","plt.ylabel('Actual Price')\n","plt.suptitle(\"Linear Regression Model\", fontsize=16)\n","plt.title('All Features')\n","plt.xlim(10.5,15), plt.ylim(10.5,15)\n","plt.text(12.48,14.25,'Slope={}'.format(round(m,2)), fontsize=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2KIyWLI-U3MK"},"source":["#### Outlier Discussion\n","Several apparent outliers were apparent in the first run.  With this information, we elected to drop IDs [1298, 523, 1100, 533] from the training data.\n","\n","Lets attempt to handle with some python libraries\n","\n","[Cited Work](https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/)\n","\n"]},{"cell_type":"code","metadata":{"id":"gYl07c8kC2_P"},"source":["train_vectors, dev_vectors, train_SalePrice, dev_SalePrice, test_vectors = get_data(data, False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUWzZrDdVSy9"},"source":["mae = 1\n","\n","# summarize the shape of the updated training dataset\n","print('Original Shapes: ', train_vectors.shape, train_SalePrice.shape)\n","\n","while mae > .097:\n","  # Show Mean Abs Error for original model\n","  mae = sklearn.metrics.mean_absolute_error(np.log(dev_SalePrice), y_pred)\n","  \n","  X_train, y_train = train_vectors, train_SalePrice\n","  X_test, y_test = dev_vectors, dev_SalePrice\n","\n","  # identify outliers in the training dataset\n","  iso = IsolationForest(contamination=.09)\n","  yhat = iso.fit_predict(X_train)\n","  mask = yhat != -1\n","\n","  X_train, y_train = X_train[mask], y_train[mask]\n","\n","  # fit the model\n","  model = LinearRegression()\n","  model.fit(X_train, np.log(y_train))\n","\n","  # evaluate the model\n","  yhat = model.predict(X_test)\n","\n","  # evaluate predictions\n","  mae = sklearn.metrics.mean_absolute_error(np.log(y_test), yhat)\n","\n","  print('Processed MAE: %.3f' % mae)\n","\n","# summarize the shape of the updated training dataset\n","print(X_train.shape, y_train.shape)\n","\n","# evaluate the model\n","print(f\" Score {model.score(X_test, np.log(y_test))}\")\n","print(f\" MSE {sklearn.metrics.mean_squared_error(yhat, np.log(dev_SalePrice))}\")\n","print(f\" RMSE {np.sqrt(sklearn.metrics.mean_squared_error(np.exp(yhat), dev_SalePrice))}\")\n","\n","if submission:\n","\n","  # evaluate the model\n","  y_pred = model.predict(test_vectors)\n","  submission_file(np.exp(y_pred), 'isolation_forest_log.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vQe1ylom6ueT"},"source":["#### Linear Model with all Features and outliers removed\n","Rank: 2002    Score:  14223.31 ***Most optimal model***"]},{"cell_type":"code","metadata":{"id":"xki-osW79vd3"},"source":["# Plot set up\n","plt.style.use('ggplot')\n","plt.figure(figsize=(10,8))\n","\n","# Set x and y variable\n","x, y = yhat, np.log(y_test)\n","\n","# Scatter plot - fitted vs. actuals\n","plt.scatter(x, y, alpha=.7,\n","            color='b') #alpha helps to show overlapping data\n","\n","# Line of best fit\n","m, b = np.polyfit(x, y, 1)\n","plt.plot(x, m*x + b)\n","\n","# Plot set up\n","plt.xlabel('Predicted Price')\n","plt.ylabel('Actual Price')\n","plt.suptitle(\"Linear Regression Model\", fontsize=16)\n","plt.title('All Features')\n","plt.xlim(10.5,15.5), plt.ylim(10.5,15.5)\n","plt.text(12.48,14.25,'Slope={}'.format(round(m,2)), fontsize=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkv39qT_FXEK"},"source":["### Linear Model: Selective Features"]},{"cell_type":"code","metadata":{"id":"Pyt1yEQTFajI"},"source":["# Correlation Matrix\n","corr = data.corr()\n","highest_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5] \n","plt.figure(figsize=(10,10))\n","g = sns.heatmap(data[highest_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YtyXGAUPFddo"},"source":["df = data[highest_corr_features]\n","df[\"OverallQual\"] = data[\"OverallQual\"]\n","\n","train_vectors, dev_vectors, train_SalePrice, dev_SalePrice, test_vectors = get_data(df, False)\n","\n","# Linear Regression, all\n","reg = LinearRegression()\n","reg.fit(train_vectors, np.log(train_SalePrice))\n","\n","y_pred = reg.predict(dev_vectors)\n","print(reg.score(dev_vectors, np.log(dev_SalePrice)))\n","print(sklearn.metrics.mean_squared_error(y_pred, np.log(dev_SalePrice)))\n","print(np.sqrt(sklearn.metrics.mean_squared_error(y_pred, np.log(dev_SalePrice))))\n","\n","X_train, y_train, X_test, y_test = train_vectors, train_SalePrice, dev_vectors, dev_SalePrice\n","\n","# Show Mean Abs Error for original model\n","mae = sklearn.metrics.mean_absolute_error(np.log(y_test), y_pred)\n","print('Original MAE: %.3f' % mae)\n","\n","# summarize the shape of the updated training dataset\n","print('Original Shapes: ', train_vectors.shape, train_SalePrice.shape)\n","\n","# identify outliers in the training dataset\n","iso = IsolationForest(contamination=0.1)\n","yhat = iso.fit_predict(X_train)\n","mask = yhat != -1\n","\n","X_train, y_train = X_train[mask], y_train[mask]\n","\n","# summarize the shape of the updated training dataset\n","print(X_train.shape, y_train.shape)\n","\n","# fit the model\n","model = LinearRegression()\n","model.fit(X_train, np.log(y_train))\n","\n","# evaluate the model\n","yhat = model.predict(X_test)\n","\n","# evaluate predictions\n","mae = sklearn.metrics.mean_absolute_error(np.log(y_test), yhat)\n","print('MAE: %.3f' % mae)\n","\n","# evaluate the model\n","yhat = model.predict(test_vectors)\n","submission_file(np.exp(yhat), 'linear_regression_selective.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8GmX6H-tZTn"},"source":["np.exp(.14)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wRlmIH9gqGPf"},"source":["# Plot set up\n","plt.style.use('ggplot')\n","plt.figure(figsize=(10,8))\n","\n","# Scatter plot - fitted vs. actual\n","plt.scatter(y_pred, np.log(dev_SalePrice), alpha=.7,\n","            color='b') #alpha helps to show overlapping data\n","\n","# Line of best fit\n","x, y = y_pred, np.log(dev_SalePrice).to_numpy()\n","m, b = np.polyfit(x, y, 1)\n","plt.plot(x, m*x + b)\n","\n","# Plot set up\n","plt.xlabel('Predicted Price')\n","plt.ylabel('Actual Price')\n","plt.suptitle(\"Linear Regression Model\", fontsize=16)\n","plt.title('Selective Features')\n","plt.xlim(10.5,15), plt.ylim(10.5,15)\n","plt.text(12.48,14.25,'Slope={}'.format(round(m,2)), fontsize=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vAQO64IcoOf"},"source":["sns.pairplot(data=data,\n","                  y_vars=['SalePrice'],\n","                  x_vars=df.drop(['SalePrice'],axis=1,inplace=False).columns)\n","\n","new_df = df\n","\n","new_df.TotalBsmtSF = np.log(new_df.TotalBsmtSF)\n","\n","sns.pairplot(data=data,\n","                  y_vars=['SalePrice'],\n","                  x_vars=df.drop(['SalePrice'],axis=1,inplace=False).columns)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xfyz0U9kPCjZ"},"source":["## Lasso:\n"]},{"cell_type":"code","metadata":{"id":"MlQniuzHcErb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618361053938,"user_tz":420,"elapsed":225427,"user":{"displayName":"Bo Qian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggjf7nL8zWgF3wUCCZ0sL1ZpiPinUEypQhrlssI=s64","userId":"03621079866129484645"}},"outputId":"3d28551f-f95b-44a5-9164-0dfadfe89321"},"source":["train_vectors, dev_vectors, train_SalePrice, dev_SalePrice, test_vectors = get_data(data, False)\n","\n","# generate a range of alpha\n","alpha=np.arange(0.01, 1, 0.0005)\n","score_best = 0\n","i_best = 0\n","# initiate a lasso model using different alpha\n","for i in alpha:\n","  las = Lasso(alpha=i)\n","  las_fit = las.fit(train_vectors, np.log(train_SalePrice))\n","  score = las.score(dev_vectors, np.log(dev_SalePrice))\n","  # pick up the best score and its coresponding alpha\n","  if score > score_best:\n","    score_best = score\n","    alpha_best = i\n","# print out the best alpha and its score\n","# print(alpha_best,score_best)\n","print('score:',score_best)\n","\n","# predict on dev data\n","las_best = Lasso(alpha=alpha_best)\n","las_best_fit = las_best.fit(train_vectors, np.log(train_SalePrice))\n","las_pred = las_best.predict(dev_vectors)\n","# print out the MSE and RMSE\n","print('MSE:',sklearn.metrics.mean_squared_error(las_pred, np.log(dev_SalePrice)))\n","print('RMSE:',np.sqrt(sklearn.metrics.mean_squared_error(las_pred, np.log(dev_SalePrice))))\n","\n","if submission:\n","\n","  train_vectors, train_SalePrice, test_vectors = get_data(data, True)\n","\n","  las_best_fit = las_best.fit(train_vectors, np.log(train_SalePrice))\n","  y_pred = las_best.predict(test_vectors)\n","  submission_file(np.exp(y_pred), 'lasso_log.csv')"],"execution_count":55,"outputs":[{"output_type":"stream","text":["score: 0.8602606719733411\n","MSE: 0.021325342283049654\n","RMSE: 0.1460319906152404\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Fg5gkBvcFtz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618361053951,"user_tz":420,"elapsed":225437,"user":{"displayName":"Bo Qian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggjf7nL8zWgF3wUCCZ0sL1ZpiPinUEypQhrlssI=s64","userId":"03621079866129484645"}},"outputId":"7d72a904-7c0b-4e27-fe07-a73d8f6e074f"},"source":["# print out the number of features selected by the model\n","selected_feat = train_vectors.columns[(SelectFromModel(las_best,prefit=True).get_support())]\n","print('total features: {}'.format((train_vectors.shape[1])))\n","print('selected features: {}'.format(len(selected_feat)))"],"execution_count":56,"outputs":[{"output_type":"stream","text":["total features: 349\n","selected features: 26\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"81ehZMiKPU9f"},"source":["## Enet:\n"]},{"cell_type":"code","metadata":{"id":"weSxva2scIis","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618361310244,"user_tz":420,"elapsed":481728,"user":{"displayName":"Bo Qian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggjf7nL8zWgF3wUCCZ0sL1ZpiPinUEypQhrlssI=s64","userId":"03621079866129484645"}},"outputId":"a51fe3f6-bba7-40ab-98de-c6daaf0a8fce"},"source":["train_vectors, dev_vectors, train_SalePrice, dev_SalePrice, test_vectors = get_data(data, False)\n","\n","# generate a range of alpha\n","alpha=np.arange(0.01, 1, 0.0005)\n","score_best = 0\n","i_best = 0\n","# initiate a ENet model using different alpha\n","for i in alpha:\n","  enet = ElasticNet(alpha=i)\n","  enet_fit = enet.fit(train_vectors, np.log(train_SalePrice))\n","  score = enet.score(dev_vectors, np.log(dev_SalePrice))\n","  # pick up the best score and its coresponding alpha\n","  if score > score_best:\n","    score_best = score\n","    alpha_best = i\n","# print out the best alpha and its score\n","# print(alpha_best,score_best)\n","print('score:',score_best)\n","\n","# predict on dev data\n","enet_best = ElasticNet(alpha=alpha_best)\n","enet_best_fit = enet_best.fit(train_vectors, np.log(train_SalePrice))\n","enet_pred = enet_best.predict(dev_vectors)\n","# print out the MSE and RMSE\n","print('MSE:',sklearn.metrics.mean_squared_error(enet_pred, np.log(dev_SalePrice)))\n","print('RMSE:',np.sqrt(sklearn.metrics.mean_squared_error(enet_pred, np.log(dev_SalePrice))))\n","\n","if submission:\n","\n","  train_vectors, train_SalePrice, test_vectors = get_data(data, True)\n","\n","  net_best_fit = enet_best.fit(train_vectors, np.log(train_SalePrice))\n","  y_pred = enet_best.predict(test_vectors)\n","  submission_file(np.exp(y_pred), 'enet_log.csv')"],"execution_count":57,"outputs":[{"output_type":"stream","text":["score: 0.8873165583633741\n","MSE: 0.017196397009112994\n","RMSE: 0.13113503349262925\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-_1dBhsRPVC5"},"source":["## XGBoost:"]},{"cell_type":"code","metadata":{"id":"MhXWXBO4adnF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"712e3c1c-b9e9-4729-c01e-050f416b5db7"},"source":["regressor=xgboost.XGBRegressor()\n","\n","booster=['gbtree','gblinear']\n","base_score=[0.25,0.5,0.75,1]\n","\n","## Hyper Parameter Optimization\n","n_estimators = [100, 500, 900, 1100, 1500]\n","max_depth = [2, 3, 5, 10, 15]\n","booster=['gbtree','gblinear']\n","learning_rate=[0.05,0.1,0.15,0.20]\n","min_child_weight=[1,2,3,4]\n","\n","# Define the grid of hyperparameters to search\n","hyperparameter_grid = {\n","    'n_estimators': n_estimators,\n","    'max_depth':max_depth,\n","    'learning_rate':learning_rate,\n","    'min_child_weight':min_child_weight,\n","    'booster':booster,\n","    'base_score':base_score\n","    }\n","\n","# Set up the random search with 4-fold cross validation\n","random_cv = GridSearchCV(estimator=regressor,\n","            param_grid=hyperparameter_grid,\n","            cv=5, \n","            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n","            verbose = 5, \n","            return_train_score = True\n","            )\n","\n","if submission:\n","  # comment out when not fitting model ***TAKES A WHILE***\n","  random_cv.fit(train_vectors, train_SalePrice)\n","  filename = 'xgboost_model.pkl'\n","  pickle.dump(regressor, open(filename, 'wb'))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 5 folds for each of 3200 candidates, totalling 16000 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n","[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   23.5s\n","[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  3.9min\n","[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 11.2min\n","[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed: 26.9min\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"RHHJzS3CtO5Y"},"source":["sns.distplot(data['FirstFlrSF'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LHbeLnYZtT9d"},"source":["from scipy.special import boxcox1p\n","from scipy.stats import boxcox_normmax\n","\n","data['FirstFlrSF'] = boxcox1p(data['FirstFlrSF'], boxcox_normmax(data['FirstFlrSF'] + 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9g5xoiPjtwpe"},"source":["sns.distplot(data['FirstFlrSF'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZLqxdPrsGTo"},"source":["# Fitting model (Advanced approach)"]},{"cell_type":"code","metadata":{"id":"xSD5r2zas8uR"},"source":["train_vectors, dev_vectors, train_SalePrice, dev_SalePrice, test_vectors = get_data(data, True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7sRuda2s1k_"},"source":["def overfit_reducer(df):\n","    \"\"\"\n","    This function takes in a dataframe and returns a list of features that are overfitted.\n","    \"\"\"\n","    overfit = []\n","    for i in df.columns:\n","        counts = df[i].value_counts()\n","        zeros = counts.iloc[0]\n","        if zeros / len(df) * 100 > 99.94:\n","            overfit.append(i)\n","    overfit = list(overfit)\n","    return overfit\n","\n","\n","overfitted_features = overfit_reducer(data)\n","\n","X = X.drop(overfitted_features, axis=1)\n","X_sub = X_sub.drop(overfitted_features, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgbF5DMiruCF"},"source":["kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n","\n","def rmsle(y, y_pred):\n","    return np.sqrt(mean_squared_error(y, y_pred))\n","\n","def cv_rmse(model, X=X):\n","    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n","    return (rmse)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RV62BAfrr0H4"},"source":["alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n","alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n","e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n","e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQFdcoyFr16R"},"source":["ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n","lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n","                                              alphas=alphas2, \n","                                              random_state=42, \n","                                              cv=kfolds))\n","elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \n","svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yaJOnK-Wr3pw"},"source":["gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)                             "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptzxq5pZr5I3"},"source":["lightgbm = LGBMRegressor(objective='regression', \n","                                       num_leaves=4,\n","                                       learning_rate=0.01, \n","                                       n_estimators=5000,\n","                                       max_bin=200, \n","                                       bagging_fraction=0.75,\n","                                       bagging_freq=5, \n","                                       bagging_seed=7,\n","                                       feature_fraction=0.2,\n","                                       feature_fraction_seed=7,\n","                                       verbose=-1,\n","                                       )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_u5kFJjgr6qn"},"source":["xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n","                                     max_depth=3, min_child_weight=0,\n","                                     gamma=0, subsample=0.7,\n","                                     colsample_bytree=0.7,\n","                                     objective='reg:linear', nthread=-1,\n","                                     scale_pos_weight=1, seed=27,\n","                                     reg_alpha=0.00006)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oRNInH8r7-p"},"source":["stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, xgboost, lightgbm),\n","                                meta_regressor=xgboost,\n","                                use_features_in_secondary=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EB54_x9r-9R"},"source":["# score = cv_rmse(stack_gen)\n","# print(\"Stack: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNjFgsS4sAq5"},"source":["score = cv_rmse(ridge)\n","print(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n","\n","score = cv_rmse(lasso)\n","print(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n","\n","score = cv_rmse(elasticnet)\n","print(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n","\n","score = cv_rmse(svr)\n","print(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n","\n","score = cv_rmse(lightgbm)\n","print(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n","\n","# score = cv_rmse(gbr)\n","# print(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n","\n","score = cv_rmse(xgboost)\n","print(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7U3X4IYqMS67"},"source":["# Conclusion\n","\n","Considerable EDA was required in this project, with a fair amount of feature engineering as well.  Although there is certainly room to address distribution and skewness of additional features, we are quite satisfied with our linear model regression techniques.  Although we only ranked ~ 2000, because of the way Kaggle scores only half the test set prior to the close of the competition, we feel confident a good portion of the higher-ranked models will suffer from overfitting.\n","\n","Overall, we included six models in our projectLinearRegression, LR with outlier removed, LR with features with correlation above 0.5, Lasso and Enet, as well as XGBoost. \n","\n","Additional model comparison with pros and cons can be found in the associated slide deck.\n"]},{"cell_type":"code","metadata":{"id":"ftoj5r7xaVEs"},"source":[""],"execution_count":null,"outputs":[]}]}