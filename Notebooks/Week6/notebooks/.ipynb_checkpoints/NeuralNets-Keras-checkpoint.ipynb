{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NeuralNets-Keras-2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMyZTmwl_nmC",
        "colab_type": "text"
      },
      "source": [
        "# Digit Classification with Neural Networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Oe-7qpo_nmH",
        "colab_type": "text"
      },
      "source": [
        "Interest in neural networks, and in particular those with architechures that support deep learning, has been surging in recent years.  \n",
        "\n",
        "In this notebook we will be revisiting the problem of digit classification on the MNIST data.  In doing so, we will introduce a new Python library, Keras, for working with neural networks.  Keras is a popular choice for neural networks as the same code can be run on either CPUs or GPUs.  GPUs greatly speed up the training and prediction, and is readily available. Amazon even offers GPU machines on EC2.  \n",
        "\n",
        "In part 1, we'll introduce Keras, and refresh ourselves on the MNIST dataset.  In part 2, we'll create a multi-layer neural network with a simple architechure, and train it using backpropagation.  Part 3 will introduce the convolutional architechure, which can be said to be doing 'deep learning' (also called feature learning or representation learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gm-4OXk_nmK",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW1K1MzB_nmM",
        "colab_type": "code",
        "outputId": "d2900340-e28f-4b03-bd61-f3f3a294e4de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%matplotlib inline\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import time\n",
        "\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "\n",
        "np.random.seed(0)\n",
        "print (\"OK\")\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version\", tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "OK\n",
            "Tensorflow version 2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpvjynH8dKYJ",
        "colab_type": "code",
        "outputId": "8fd5146b-eb74-43b8-c6b8-5c4cbb0b8308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.random.rand(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.71518937])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzOQetfr_nmW",
        "colab_type": "text"
      },
      "source": [
        "Now for MNIST data..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxxtGG7h_nmX",
        "colab_type": "code",
        "outputId": "a8e83af0-f7bb-4b51-bc16-d887b1fad626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Repeating steps from Project 1 to prepare mnist dataset. \n",
        "X, Y = fetch_openml(name='mnist_784', return_X_y=True, cache=False)\n",
        "X = X / 255.0\n",
        "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
        "X, Y = X[shuffle], Y[shuffle]\n",
        "numExamples = 2000\n",
        "test_data, test_labels = X[70000-numExamples:], Y[70000-numExamples:]\n",
        "train_data, train_labels = X[:numExamples], Y[:numExamples]\n",
        "numFeatures = train_data[1].size\n",
        "numTrainExamples = train_data.shape[0]\n",
        "numTestExamples = test_data.shape[0]\n",
        "print ('Features = %d' %(numFeatures))\n",
        "print ('Train set = %d' %(numTrainExamples))\n",
        "print ('Test set = %d' %(numTestExamples))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features = 784\n",
            "Train set = 2000\n",
            "Test set = 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTEQ2YXc_nme",
        "colab_type": "text"
      },
      "source": [
        "Looking ahead to working with neural networks, let's prepare one additional variation of the label data.  Let's make these labels, rather than each being an integer value from 0-9, be a set of 10 binary values, one for each class.  This is sometimes called a **1-of-n encoding**, and it makes working with Neural Networks easier, as there will be one output node for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifjzbScT_nmf",
        "colab_type": "code",
        "outputId": "fe2ef600-f0e7-4565-9ad4-7f14c38c64d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def binarizeY(data):\n",
        "    binarized_data = np.zeros((data.size, 10))\n",
        "    for j in range(0, data.size):\n",
        "        feature = data[j:j+1]\n",
        "        i = feature.astype(np.int64) \n",
        "        binarized_data[j, i] = 1\n",
        "    return binarized_data\n",
        "train_labels_b = binarizeY(train_labels)\n",
        "test_labels_b = binarizeY(test_labels)\n",
        "numClasses = train_labels_b[1].size\n",
        "print ('Classes = %d' %(numClasses))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classes = 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DlyxLB__nmk",
        "colab_type": "text"
      },
      "source": [
        "Lets start with a KNN model to establish a baseline accuracy.\n",
        "\n",
        "Exercise: You've seen a number of different classification algorithms (e.g. naive bayes, decision trees, random forests, logistic regression) at this point.  How does KNN scalability and accuracy with respect to the size of the training dataset compare to those other algorithms?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jIUKOIJ_nmm",
        "colab_type": "code",
        "outputId": "8799d2ec-f82f-47af-9041-18af409326e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "neighbors = 1\n",
        "knn = KNeighborsClassifier(neighbors)\n",
        "# we'll be waiting quite a while if we use 60K examples, so let's cut it down.  You may want to run the full 60K on your own later to see what the accuracy is.\n",
        "mini_train_data, mini_train_labels = X[:numExamples], Y[:numExamples] \n",
        "start_time = time.time()\n",
        "knn.fit(mini_train_data, mini_train_labels)\n",
        "print ('Train time = %.2f' %(time.time() - start_time))\n",
        "start_time = time.time()\n",
        "accuracy = knn.score(test_data, test_labels)\n",
        "print ('Accuracy = %.4f' %(accuracy))\n",
        "print ('Prediction time = %.2f' %(time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train time = 0.16\n",
            "Accuracy = 0.9135\n",
            "Prediction time = 8.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S50DwT5G_nms",
        "colab_type": "text"
      },
      "source": [
        "Alright, now that we have a simple baseline, let's start working in Keras.  Before we jump to multi-layer neural networks though, **let's train a logistic regression model** to make certain we're using Keras correctly. \n",
        "\n",
        "Recall from Josh's regression lecture the four key components: (1) parameters, (2) model, (3) cost function, and (4) objective. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUltZGGJ_nmu",
        "colab_type": "text"
      },
      "source": [
        "Two notes relevant at this point:\n",
        "\n",
        "First, logistic regression can be thought of as a neural network with no hidden layers. The output values are just the dot product of the inputs and the edge weights.\n",
        "\n",
        "Second, we have 10 classes. We can either train separate one vs all classifiers using sigmoid activation, which would be a hassle, or we can use the softmax activation, which is essentially a multi-class version of sigmoid. We'll use Keras's built-in implementation of softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDAM5VlY_nmw",
        "colab_type": "text"
      },
      "source": [
        "The objective is minimize the cost, and to do that we'll use batch gradient descent.\n",
        "\n",
        "Exercise: What are the differences between batch, stochastic, and mini-batch gradient descent?  What are the implications of each for working on large datasets?\n",
        "\n",
        "Exercise: Do you recall from Josh's lecture what the gradient is for beta in logistic regression?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOKNAWr6_nmw",
        "colab_type": "code",
        "outputId": "efdfcc3b-d6cf-4aca-b4ae-0fcfdc2c9df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "## Model\n",
        "model = Sequential() \n",
        "model.add(Dense(10, input_dim=784, activation='softmax')) \n",
        "# How many weights will we have...?\n",
        "\n",
        "## Cost function & Objective (and solver)\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "start_time = time.time()\n",
        "history = model.fit(train_data, train_labels_b, shuffle=False, batch_size=numTrainExamples, verbose=0, epochs=50) \n",
        "print ('Train time = %.2f' %(time.time() - start_time))\n",
        "score = model.evaluate(test_data, test_labels_b, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train time = 1.26\n",
            "Test score: 1.9152084064483643\n",
            "Test accuracy: 0.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zAHpHtS_nm1",
        "colab_type": "text"
      },
      "source": [
        "Exercise:  What do you expect to happen if we convert batch gradient descent to **stochastic gradient descent**?  Why?\n",
        "\n",
        "Let's try it..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtYcxwD4_nm3",
        "colab_type": "code",
        "outputId": "47749b0b-c824-4994-d252-e8b0fa48d4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "## Model\n",
        "model = Sequential() \n",
        "model.add(Dense(10, input_dim=784, activation='softmax')) \n",
        "\n",
        "## Cost function & Objective (and solver)\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "start_time = time.time()\n",
        "history = model.fit(train_data, train_labels_b, shuffle=False, batch_size=1,verbose=0, epochs=50) \n",
        "print ('Train time = %.2f' %(time.time() - start_time))\n",
        "score = model.evaluate(test_data, test_labels_b, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train time = 120.34\n",
            "Test score: 0.455361901819706\n",
            "Test accuracy: 0.8755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlStyVtwiDA-",
        "colab_type": "code",
        "outputId": "daeb1900-f6a9-4304-ae08-b8c973c6514d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWEP6UGKTN9P",
        "colab_type": "text"
      },
      "source": [
        "What makes the second one \"Stochastic Gradient Descent\"? Is it fair to compare the last two attempts? Why?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in5-lrb5_nm9",
        "colab_type": "text"
      },
      "source": [
        "# PART 2: Multi-layer Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqs4r38K_nm-",
        "colab_type": "text"
      },
      "source": [
        "---------\n",
        "\n",
        "Let's take our implementation of logistic regression (which recall is in fact a single layer neural network), and add a hidden layer, making it a two layer neural network.  Because we have a hidden layer, we will now train the model using backpropagation.\n",
        "\n",
        "Exercise: How do you expect this model to compare to KNN and logistic regression in terms of train time and accuracy?  Why?\n",
        "\n",
        "Let's try it out..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fkqRjZ3_nm_",
        "colab_type": "code",
        "outputId": "7bc36e70-5461-4d50-ca9a-d969119cee64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "## Model\n",
        "model = Sequential() \n",
        "model.add(Dense(units=50, input_dim=784, activation='sigmoid')) \n",
        "model.add(Dense(units=10, input_dim=50, activation='softmax')) \n",
        "\n",
        "## Cost function & Objective (and solver)\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, train_labels_b, shuffle=False, batch_size=10,verbose=0, epochs=50) \n",
        "score = model.evaluate(test_data, test_labels_b, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.412759975194931\n",
            "Test accuracy: 0.8855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KybfgqSh_nnG",
        "colab_type": "text"
      },
      "source": [
        "--------\n",
        "\n",
        "As interest in networks with more layers and more complicated architechures has increased, a couple of tricks have emerged and become standard practice.  Let's look at two of those--rectifier activation and dropout noise.\n",
        "\n",
        "Exercise:  We saw an improvement from adding a hidden layer.  What do you expect to happen if a second hidden layer was added?  \n",
        "\n",
        "Let's try it..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-ZSLk32_nnH",
        "colab_type": "code",
        "outputId": "07ec70df-92fb-417c-b47b-9af75dbb0bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "## Model\n",
        "model = Sequential() \n",
        "model.add(Dense(units=20, input_dim=784, activation='sigmoid'))\n",
        "model.add(Dense(units=20, input_dim=20, activation='sigmoid'))\n",
        "model.add(Dense(units=10, input_dim=20, activation='softmax')) \n",
        "\n",
        "## Cost function & Objective (and solver)\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, train_labels_b, shuffle=False, batch_size=100,verbose=0, epochs=50) \n",
        "score = model.evaluate(test_data, test_labels_b, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 2.245059772491455\n",
            "Test accuracy: 0.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiATYL17_nnL",
        "colab_type": "text"
      },
      "source": [
        "#### Activation Revisted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT-NFVy__nnM",
        "colab_type": "text"
      },
      "source": [
        "Let's look at a recent idea around activation closely associated with deep learning.  In 2010, in a paper published at NIPS (https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf), Yoshua Bengio showed that rectifier activation works better empirically than sigmoid activation when used in the hidden layers.  \n",
        "\n",
        "The rectifier activation is simple: f(x)=max(0,x).  Intuitively, the difference is that as a sigmoid activated node approaches 1 it stops learning even if error continues to be propagated to it, whereas the rectifier activated node continue to learn (at least in the positive direction).  Rectifiers also speed up training.\n",
        "\n",
        "Although the paper was published in 2010, the technique didn't gain widespread adoption until 2012 when members of Hinton's group spread the word, including with this Kaggle entry: http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/\n",
        "\n",
        "Let's change the activation in our 2 layer network to rectifier and see what happens..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-HXC5b5_nnN",
        "colab_type": "code",
        "outputId": "53541782-021b-46d2-f109-a417dddd81e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "## Model\n",
        "model = Sequential() \n",
        "model.add(Dense(units=30, input_dim=784, activation='relu')) \n",
        "model.add(Dense(units=10, input_dim=30, activation='softmax')) \n",
        "\n",
        "## Cost function & Objective (and solver)\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, train_labels_b, shuffle=False, batch_size=10,verbose=0, epochs=50) \n",
        "score = model.evaluate(test_data, test_labels_b, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.3647029350399971\n",
            "Test accuracy: 0.8965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNUuVWD4_nnR",
        "colab_type": "text"
      },
      "source": [
        "#### Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Z00VNC_nnS",
        "colab_type": "text"
      },
      "source": [
        "Previously when working with the MNIST data we saw a benefit in generalization from adding noise to the training data.  Let's try that again here, however this time with a trick for adding noise called 'Dropouts'.  The idea with dropouts is that instead of (or in addition to) adding noise to our inputs, we add noise by having each node return 0 with a certain probability during training.  This trick both improves generalization in large networks and speeds up training.\n",
        "\n",
        "Hinton introduced the idea in 2012 and gave an explanation of why it's similar to bagging (http://arxiv.org/pdf/1207.0580v1.pdf)\n",
        "\n",
        "Let's give it a try..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wFAN-ej_nnT",
        "colab_type": "code",
        "outputId": "dd368b18-d140-4c18-fe34-bc18028cd593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "## Model\n",
        "model = Sequential() \n",
        "model.add(Dense(units=50, input_dim=784, activation='relu')) \n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=10, input_dim=50, activation='softmax')) \n",
        "\n",
        "## Cost function & Objective (and solver)\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, train_labels_b, shuffle=False, batch_size=10,verbose=0, epochs=50) \n",
        "score = model.evaluate(test_data, test_labels_b, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.3247391206622124\n",
            "Test accuracy: 0.896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9efslQ0dNgV",
        "colab_type": "text"
      },
      "source": [
        "# PART 3: Convolution Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmIAtUpH7oZw",
        "colab_type": "code",
        "outputId": "8de16d71-99a3-4beb-e369-00c8520f655e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "## Model\n",
        "\n",
        "x_train_28x28 = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
        "x_test_28x28 = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28, 28, 1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=50, input_dim=128, activation='relu')) \n",
        "model.add(Dense(units=10, input_dim=50, activation='softmax')) \n",
        "\n",
        "## Cost function & Objective (and solver)\n",
        "sgd = optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train_28x28, train_labels_b, batch_size=25, epochs=10, verbose=1, validation_data=(x_test_28x28, test_labels_b))\n",
        "score = model.evaluate(x_test_28x28, test_labels_b, verbose=0)\n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2000 samples, validate on 2000 samples\n",
            "Epoch 1/10\n",
            "2000/2000 [==============================] - 7s 4ms/sample - loss: 2.1928 - accuracy: 0.2845 - val_loss: 1.9140 - val_accuracy: 0.4045\n",
            "Epoch 2/10\n",
            "2000/2000 [==============================] - 7s 3ms/sample - loss: 1.2433 - accuracy: 0.6645 - val_loss: 0.7165 - val_accuracy: 0.7765\n",
            "Epoch 3/10\n",
            "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.6212 - accuracy: 0.8025 - val_loss: 0.4842 - val_accuracy: 0.8540\n",
            "Epoch 4/10\n",
            "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.5286 - accuracy: 0.8360 - val_loss: 0.4470 - val_accuracy: 0.8585\n",
            "Epoch 5/10\n",
            "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.4571 - accuracy: 0.8555 - val_loss: 0.3948 - val_accuracy: 0.8830\n",
            "Epoch 6/10\n",
            "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.4060 - accuracy: 0.8725 - val_loss: 0.3897 - val_accuracy: 0.8880\n",
            "Epoch 7/10\n",
            "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.3924 - accuracy: 0.8765 - val_loss: 0.3665 - val_accuracy: 0.8910\n",
            "Epoch 8/10\n",
            "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.3694 - accuracy: 0.8850 - val_loss: 0.3519 - val_accuracy: 0.8950\n",
            "Epoch 9/10\n",
            "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.3406 - accuracy: 0.8955 - val_loss: 0.3549 - val_accuracy: 0.8995\n",
            "Epoch 10/10\n",
            "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.3096 - accuracy: 0.9085 - val_loss: 0.3586 - val_accuracy: 0.8960\n",
            "Test score: 0.35862948083877566\n",
            "Test accuracy: 0.896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz1WKPHNdU5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}